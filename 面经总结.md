## 一、总结
|  类型   | 题目  |  思路          |  公司 |  注意事项   | 时间  |
|  -     | -     | -              |  -   |  -        | -   |
|ml|svm相比于LR或者Perceptron 优势在哪里?|?|字节AI lab|?|21.12|
|ml|如果把正负样本采样的比例从 1:1 改成 1:2，AUC 的值会如何发生变化，如果把CTR输出的模型里面分数比较低的样本去掉，从新计算 AUC ，AUC 的值会如何变化|?|抖音电商|2022.03|
| ml  | 利用pytorch拟合log函数 | 回归  | 商汤 | pytorch  | 6月 |
|ml|手撕机器学习中的K-means算法|python|微软|优化时间空间|5月|
|nlp|self attention的  pre norm和post norm| ? | 字节AILab | ? | 5月|
|ml|集成学习BAGGING和BOOSTING。RF、XGBOOST、LIGHTGBM的特点和区别| ? | paypal | ? | 21.12|
|ml|生成式模型和判别式模型有什么区别，有哪些模型?| ? | 字节AILab | ? | 21.12|
| nlp  | 利用pytorch实现transformer | qkv  | 拼多多,B站 | pytorch  | 6月 |
| nlp  | 手写beamsearch | dp  | 微软 | python,dp  | 6月 |
|数学| 蒙特卡罗方法估计Pi值 | r=0.5,外切正方形w=1, pi/4 = M/N => pi = 4M/N | 抖音电商| https://www.nowcoder.com/discuss/925710?source_id=discuss_experience_nctrack&channel=-1|2022.03|
|nlp|TFIDF、Word2Vec和BERT| ? | paypal | ? | 21.12|
|nlp|CNN、RNN、LSTM和Transformer的特点和区别；|?|paypal|?|21.12|
|nlp|Word2vec的两种训练目标是什么 其中skip-gram训练的loss function是什么?|?|字节AI lab|?|21.12|
|nlp|怎么去判断生成的结束位置?|?|字节AI lab|?|21.12|
|编译原理|编译器如何把一段c语言代码转成汇编语言？需要经历哪些步骤，有哪些中间文件？|?|字节AI lab|?|21.12|

## 二、详细解读

### 2.1 ml相关
#### 2.1.1 LR和SVM的异同
- 相同点
  - 都可以处理分类问题
  - 都可以加不同的正则化项
  - 都可以用来作非线性回归
  - 都是线性模型
  - 都属于判别模型

- 不同点
  - LR是参数模型，SVM是非参数模型
  - 逻辑回归是logistical loss，svm是hinge loss(max(0, 1-t*y), t=0,1)
  - 逻辑回归模型更简单，好理解，大规模线性分类比较方便。
  - SVM只需要计算与少数几个支持向量的距离，可以大大简化模型和计算
  - svm不直接依赖数据分布，LR依赖。SVM治愈支持向量几个点有关，LR与所有点有关
  - svm依赖penalty系数，实验需要做CV
  - SVM本身是结构风险最小化模型，LR是经验风险最小化模型

## 三、参考资料

【算法岗必看系列】机器学习高频知识点-模型参数估计方法:https://www.nowcoder.com/discuss/959482?source_id=discuss_experience_nctrack&channel=-1


