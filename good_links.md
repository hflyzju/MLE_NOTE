
### 1. Google/LinkedIn等MLE面试分享 https://www.1point3acres.com/bbs/thread-781157-1-1.html
- MLE面试内容
```
Coding：平均2轮
ML coding：0到1轮
System design：0到1轮
ML fundamental （model & techniques）：平均1轮。概率和数学，放在ML fundamental里面，解一个概率题
ML system design：平均1轮
Behavior：1轮. 1poi

```

- 准备
```
ML coding：手写K-Means，手写一下KNN（e.g. 2NN)，推一下Logistic regression的loss function（不需要coding）。准备的方法也很暴力，就是背下来。
ML fundamental （model & techniques）：复习一下deep learning specialization的课程，面试过程，被面的最多的，是specialization里面improving neural networking的那门课，象dropout和batch normalization。模型的问题，我是选了3个model重点准备的，logistic regression，linear regression，neural network。之所以选这3个，是因为logistic解决classification的问题，linear解决regression的问题，而neural network是进阶版的。另外，重点看了各种metric的用法和计算方法（e.g. Precision/Recall, MSE, NDCG)
ML System design：我是以educative.IO的ML system design课程为template的，重点看了recommendation system的那一章。另外，我看了K姐的课程，地里“Learning”的页面上课程，machine learning mock interview（回放）
概率和数学：重点看了Data Science Prep的例题（https://datascienceprep.com）
```

- 相关资料
deep learning specialization：https://aman.ai/coursera-dl/

k姐，Data Scientist 炼成记录-更新完毕2018年12月 | 机器学习练成记录 - 已开新帖：https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=76429&extra=&authorid=151&page=1

k姐，机器学习`侠`练成记录 Becoming a Machine Learning Practitioner：https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=462348

k姐视频：https://blog.1point3acres.com/ds-courses/

万字长文 | 2023届校招算法岗知识总结：https://mp.weixin.qq.com/s/hYr4PDh9rgsBx18YcKX8eg

机器学习算法Python实现: https://github.com/lawlite19/MachineLearning_Python

所有搜推算法岗可能会出现的问题囊括其中: https://github.com/Jace-Yang/MLE-interview


Top 50 Machine Learning Interview Questions for 2022:https://www.interviewquery.com/p/machine-learning-interview-questions


### 2. 课程

动手学深度学习（Dive into Deep Learning）英文版：https://d2l.ai/
动手学深度学习（Dive into Deep Learning）中文版：https://zh.d2l.ai/
动手学深度学习（Dive into Deep Learning）英文版 pdf：https://d2l.ai/d2l-en.pdf
动手学深度学习（Dive into Deep Learning）英文版 课程：https://courses.d2l.ai/


### 3. 其他面试资料

geeksforgeeks machine-learning部分：https://www.geeksforgeeks.org/machine-learning/?ref=shm